[
  {
    "objectID": "modules/qc.html",
    "href": "modules/qc.html",
    "title": "Data QC",
    "section": "",
    "text": "Opportunities in Functional Genomics: A Primer on Lab and Computational Aspects"
  },
  {
    "objectID": "modules/qc.html#technology",
    "href": "modules/qc.html#technology",
    "title": "Data QC",
    "section": "Technology",
    "text": "Technology"
  },
  {
    "objectID": "modules/qc.html#quality-control",
    "href": "modules/qc.html#quality-control",
    "title": "Data QC",
    "section": "Quality Control",
    "text": "Quality Control\nThe first step in analyzing RNA-seq data is to perform quality control checks on the raw fastq files. This step is crucial to ensure that the data is of high quality and can be accurately quantified. One popular tool for quality control is FastQC, which generates various quality metrics such as per-base sequence quality, adapter contamination, and GC content.\nTo perform quality control using FastQC, run the following command:\nfastqc input.fastq\nThis will generate a HTML report that can be viewed in a web browser.\n\nAnother popular quality control program is fastp. Here is a very nice tutorial on using fastp"
  },
  {
    "objectID": "modules/align.html",
    "href": "modules/align.html",
    "title": "Alignment",
    "section": "",
    "text": "Opportunities in Functional Genomics: A Primer on Lab and Computational Aspects"
  },
  {
    "objectID": "modules/align.html#marineomics-rna-seq-panel-discussion",
    "href": "modules/align.html#marineomics-rna-seq-panel-discussion",
    "title": "Alignment",
    "section": "MarineOmics RNA-seq Panel Discussion",
    "text": "MarineOmics RNA-seq Panel Discussion"
  },
  {
    "objectID": "modules/align.html#downloading-reference",
    "href": "modules/align.html#downloading-reference",
    "title": "Alignment",
    "section": "Downloading reference",
    "text": "Downloading reference\nThis code grabs the Pacific oyster fasta file of genes and does so ignoring the fact that gannet does not have a security certificate to authenticate (--insecure). This is usually not recommended however we know the server.\n```{bash}\ncd ../data\ncurl --insecure -O https://gannet.fish.washington.edu/seashell/bu-github/nb-2023/Cgigas/data/rna.fna\n```\n\n\n\n\n\n\nNote\n\n\n\nCreating index can take some time\n\n\nThis code is indexing the file rna.fna while also renaming it as cgigas_roslin_rna.index.\n```{bash}\n/home/shared/kallisto/kallisto \\\nindex -i \\\n../data/cgigas_roslin_rna.index \\\n../data/rna.fna\n```"
  },
  {
    "objectID": "modules/align.html#downloading-sequence-reads",
    "href": "modules/align.html#downloading-sequence-reads",
    "title": "Alignment",
    "section": "Downloading sequence reads",
    "text": "Downloading sequence reads\nSequence reads are on a public server at https://gannet.fish.washington.edu/seashell/bu-github/nb-2023/Cgigas/data/nopp/\n\n\n\nSample\nSampleID\n\n\nD-control\nD54\n\n\nD-control\nD55\n\n\nD-control\nD56\n\n\nD-control\nD57\n\n\nD-control\nD58\n\n\nD-control\nD59\n\n\nD-control\nM45\n\n\nD-control\nM46\n\n\nD-control\nM48\n\n\nD-control\nM49\n\n\nD-control\nM89\n\n\nD-control\nM90\n\n\nD-desiccation\nN48\n\n\nD-desiccation\nN49\n\n\nD-desiccation\nN50\n\n\nD-desiccation\nN51\n\n\nD-desiccation\nN52\n\n\nD-desiccation\nN53\n\n\nD-desiccation\nN54\n\n\nD-desiccation\nN55\n\n\nD-desiccation\nN56\n\n\nD-desiccation\nN57\n\n\nD-desiccation\nN58\n\n\nD-desiccation\nN59\n\n\n\nThis code uses recursive feature of wget (see this weeks’ reading) to get all 24 files. Additionally as with curl above we are ignoring the fact there is not security certificate with --no-check-certificate\n```{bash}\ncd ../data \nwget --recursive --no-parent --no-directories \\\n--no-check-certificate \\\n--accept '*.fastq.gz' \\\nhttps://gannet.fish.washington.edu/seashell/bu-github/nb-2023/Cgigas/data/nopp/\n```\nThe next chunk first creates a subdirectory\nThen performs the following steps:\nThe xargs command in Unix-like systems is used to build and execute command lines from standard input. It’s often combined with other commands to perform complex operations. In your example, xargs is used twice in a pipeline that starts with the find command. Here’s a breakdown of what each part of the command does:\n\nfind ../data/*fastq.gz:\n\nThis command finds all files in the ../data/ directory (and its subdirectories) with names ending in *fastq.gz.\n\n| xargs basename -s _L001_R1_001.fastq.gz:\n\nThe output of find (paths to .fastq.gz files) is piped (|) to xargs, which then applies the basename -s _L001_R1_001.fastq.gz command to each path.\nbasename is used to strip the directory and suffix from filenames. The -s option specifies a suffix to remove.\nIn this case, basename removes the directory path and the suffix _L001_R1_001.fastq.gz from each filename.\n\n| xargs -I{} /home/shared/kallisto/kallisto quant -i ../data/cgigas_roslin_rna.index -o ../output/kallisto_01/{} -t 4 --single -l 100 -s 10 ../data/{}_L001_R1_001.fastq.gz:\n\nThe output from the previous xargs (which are now the modified filenames) is piped to another xargs command.\n-I{} is used to specify a replacement string {}. This string is replaced by each input line (filename) in the subsequent command.\nThe command /home/shared/kallisto/kallisto quant... is executed for each input line, with {} being replaced by the input filename (without path and specific suffix).\nThis part of the command runs the kallisto quant program for RNA sequence quantification, using various options and input files. The {} placeholder is replaced by the current filename (from the previous steps) in two places: for the output directory and for the input .fastq.gz file.\n\n\nIn summary, this command sequence finds .fastq.gz files, modifies their names by removing paths and a specific suffix, and then runs a kallisto quant command on each file, directing the output to a specific directory and using certain program options. This is a common pattern in bioinformatics workflows, where operations need to be applied to multiple files in an automated manner.\n\n-t 4: Use 4 threads for the computation.\n--single -l 100 -s 10: Specify that the input file contains single-end reads (–single), with an average read length of 100 (-l 100) and a standard deviation of 10 (-s 10).\n\n```{bash}\nmkdir ../output/kallisto_01\n\nfind ../data/*fastq.gz \\\n| xargs basename -s _L001_R1_001.fastq.gz | xargs -I{} /home/shared/kallisto/kallisto \\\nquant -i ../data/cgigas_roslin_rna.index \\\n-o ../output/kallisto_01/{} \\\n-t 4 \\\n--single -l 100 -s 10 ../data/{}_L001_R1_001.fastq.gz\n```\nThis command runs the abundance_estimates_to_matrix.pl script from the Trinity RNA-seq assembly software package to create a gene expression matrix from kallisto output files.\nThe specific options and arguments used in the command are as follows:\n\nperl /home/shared/trinityrnaseq-v2.12.0/util/abundance_estimates_to_matrix.pl: Run the abundance_estimates_to_matrix.pl script from Trinity.\n--est_method kallisto: Specify that the abundance estimates were generated using kallisto.\n--gene_trans_map none: Do not use a gene-to-transcript mapping file.\n--out_prefix ../output/kallisto_01: Use ../output/kallisto_01 as the output directory and prefix for the gene expression matrix file.\n--name_sample_by_basedir: Use the sample directory name (i.e., the final directory in the input file paths) as the sample name in the output matrix.\n\nAnd then there are the kallisto abundance files to use as input for creating the gene expression matrix.\n\n```{bash}\nperl /home/shared/trinityrnaseq-v2.12.0/util/abundance_estimates_to_matrix.pl \\\n--est_method kallisto \\\n    --gene_trans_map none \\\n    --out_prefix ../output/kallisto_01 \\\n    --name_sample_by_basedir \\\n    ../output/kallisto_01/D54_S145/abundance.tsv \\\n    ../output/kallisto_01/D56_S136/abundance.tsv \\\n    ../output/kallisto_01/D58_S144/abundance.tsv \\\n    ../output/kallisto_01/M45_S140/abundance.tsv \\\n    ../output/kallisto_01/M48_S137/abundance.tsv \\\n    ../output/kallisto_01/M89_S138/abundance.tsv \\\n    ../output/kallisto_01/D55_S146/abundance.tsv \\\n    ../output/kallisto_01/D57_S143/abundance.tsv \\\n    ../output/kallisto_01/D59_S142/abundance.tsv \\\n    ../output/kallisto_01/M46_S141/abundance.tsv \\\n    ../output/kallisto_01/M49_S139/abundance.tsv \\\n    ../output/kallisto_01/M90_S147/abundance.tsv \\\n    ../output/kallisto_01/N48_S194/abundance.tsv \\\n    ../output/kallisto_01/N50_S187/abundance.tsv \\\n    ../output/kallisto_01/N52_S184/abundance.tsv \\\n    ../output/kallisto_01/N54_S193/abundance.tsv \\\n    ../output/kallisto_01/N56_S192/abundance.tsv \\\n    ../output/kallisto_01/N58_S195/abundance.tsv \\\n    ../output/kallisto_01/N49_S185/abundance.tsv \\\n    ../output/kallisto_01/N51_S186/abundance.tsv \\\n    ../output/kallisto_01/N53_S188/abundance.tsv \\\n    ../output/kallisto_01/N55_S190/abundance.tsv \\\n    ../output/kallisto_01/N57_S191/abundance.tsv \\\n    ../output/kallisto_01/N59_S189/abundance.tsv\n```"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "course-fish541-2024",
    "section": "",
    "text": "This is a Quarto website.\nUse menu at left to access modules and code"
  },
  {
    "objectID": "index.html#stuff-to-read-review",
    "href": "index.html#stuff-to-read-review",
    "title": "course-fish541-2024",
    "section": "Stuff to read / review:",
    "text": "Stuff to read / review:\n\nTusk (Things you should know) - https://robertslab.github.io/tusk/\nLab Handbook - https://robertslab.github.io/resources/\nFISH 546 Bioinformatics for Environmental Sciences\n\nThis course teaches core computing skills as well as project specific approaches. Each student will be developing and completing a research project targeting journal article submission by the end of the Quarter. There will be an emphasis on developing habits that increase automation which in turn will facilitate reproducibility. The primary course platform will be centered around GitHub, with each student creating their own repositories.\nurl: https://sr320.github.io/course-fish546-2023/"
  },
  {
    "objectID": "code/hisat.html",
    "href": "code/hisat.html",
    "title": "hisat",
    "section": "",
    "text": "TLDR\n\nGenome Prep\n```{bash}\n/home/shared/hisat2-2.2.1/hisat2_extract_exons.py \\\n../data/Amil/ncbi_dataset/data/GCF_013753865.1/genomic.gtf \\\n&gt; ../output/04-Apulcra-hisat/m_exon.tab\n```\n```{bash}\n/home/shared/hisat2-2.2.1/hisat2_extract_splice_sites.py \\\n../data/Amil/ncbi_dataset/data/GCF_013753865.1/genomic.gtf \\\n&gt; ../output/04-Apulcra-hisat/m_splice_sites.tab\n```\n\n\nBuild Index\n```{bash}\n/home/shared/hisat2-2.2.1/hisat2-build \\\n../data/Amil/ncbi_dataset/data/GCF_013753865.1/GCF_013753865.1_Amil_v2.1_genomic.fna \\\n../output/GCF_013753865.1_Amil_v2.1 \\\n--exon ../output/04-Apulcra-hisat/m_exon.tab \\\n--ss ../output/04-Apulcra-hisat/m_splice_sites.tab \\\n-p 40 \\\n../data/Amil/ncbi_dataset/data/GCF_013753865.1/genomic.gtf \\\n2&gt; ../output/04-Apulcra-hisat/hisat2-build_stats.txt\n```\n\n\nQuantification\n```{bash}\nfind /home/shared/8TB_HDD_02/mewing0/clamgonads-macsamples/data/raw/*gz \\\n| xargs basename -s _R1_001.fastq.gz | xargs -I{} \\\n/home/shared/hisat2-2.2.1/hisat2 \\\n-x /home/shared/8TB_HDD_02/mewing0/clamgonads-macsamples/output/hisat/GCF_026571515.1_index \\\n-p 20 \\\n-1 /home/shared/8TB_HDD_02/mewing0/clamgonads-macsamples/data/raw/{}_R1_001.fastq.gz \\\n-2 /home/shared/8TB_HDD_02/mewing0/clamgonads-macsamples/data/raw/{}_R2_001.fastq.gz \\\n-S ../output/{}.sam\n```\n\n\nconvert SAM to BAM\n```{bash}\nfor file in ../output/*sam; do\n    base=$(basename \"$file\" .sam)\n    /home/shared/samtools-1.12/samtools view -@ 40 -bS \"$file\" | \\\n    /home/shared/samtools-1.12/samtools sort -@ 40 \\\n    -o ../output/\"$base\".bam\ndone\n```\n\n\nfeaturecounts"
  },
  {
    "objectID": "code/kallisto.html",
    "href": "code/kallisto.html",
    "title": "kallisto",
    "section": "",
    "text": "TLDR\n\nIndexing\n```{bash}\n/home/shared/kallisto/kallisto index \\\n-i ../data/Phel_transcriptome.index \\\n../data/Phel_transcriptome.fa\n```\n\n\nQuantification\n```{bash}\nmkdir ../output/kallisto_01\n\nfind ../data/*_R1.fastq.gz \\\n| xargs basename -s _R1.fastq.gz | xargs -I{} /home/shared/kallisto/kallisto quant \\\n-i ../data/Phel_transcriptome.index \\\n-o ../output/kallisto_01/{} \\\n-t 20 \\\n--fr-stranded ../data/{}_R1.fastq.gz \\\n--rf-stranded ../data/{}_R2.fastq.gz \\\n2&gt; ../output/kallisto_01/kallisto.out\n```\n\n\nStats\n\n\n\n\n\n\nmultiqc\n\n\n\nRemember this can can be used on over 140 program outputs and is a nice way to get a quick overview of your data.\n\n\n```{bash}\n/home/sam/programs/mambaforge/bin/multiqc \\\n../output/kallisto_01/kallisto.out\n```\nor\n```{bash}\neval \"$(/opt/anaconda/anaconda3/bin/conda shell.bash hook)\"\nconda activate\nwhich multiqc\n\ncd ../output/kallisto_01/\n\nmultiqc .\n```\n\n\nMerge Quant data\n```{bash}\nperl /home/shared/trinityrnaseq-v2.12.0/util/abundance_estimates_to_matrix.pl \\\n--est_method kallisto \\\n    --gene_trans_map none \\\n    --out_prefix ../output/kallisto_01 \\\n    --name_sample_by_basedir \\\n    ../output/kallisto_01/D54_S145/abundance.tsv \\\n```\n\n\nDESeq2\n```{r}\nlibrary(DESeq2)\nlibrary(tidyverse)\nlibrary(pheatmap)\nlibrary(RColorBrewer)\nlibrary(data.table)\n```\ntho you may need to install DESeq2 first\n```{r}\nif (!require(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\n\nBiocManager::install(\"DESeq2\")\n```\n```{r}\ncountmatrix &lt;- read.delim(\"../output/kallisto_01.isoform.counts.matrix\", header = TRUE, sep = '\\t')\nrownames(countmatrix) &lt;- countmatrix$X\ncountmatrix &lt;- countmatrix[,-1]\nhead(countmatrix)\n```\n```{r}\ncountmatrix &lt;- round(countmatrix, 0)\nstr(countmatrix)\n```\n```{r}\ndim(countmatrix)\ndim(deseq2.colData)\n\nlength(colnames(data))\n\ndeseq2.colData &lt;- data.frame(condition=factor(c(rep(\"control\", 12), rep(\"desicated\", 12))), \n                             type=factor(rep(\"single-read\", 24)))\nrownames(deseq2.colData) &lt;- colnames(data)\ndeseq2.dds &lt;- DESeqDataSetFromMatrix(countData = countmatrix,\n                                     colData = deseq2.colData, \n```\n```{r}\ndeseq2.dds &lt;- DESeq(deseq2.dds)\ndeseq2.res &lt;- results(deseq2.dds)\ndeseq2.res &lt;- deseq2.res[order(rownames(deseq2.res)), ]\n\n```\n```{r}\nvsd &lt;- vst(deseq2.dds, blind = FALSE)\nplotPCA(vsd, intgroup = \"condition\")\n```\n```{r}\n# Select top 50 differentially expressed genes\nres &lt;- results(deseq2.dds)\nres_ordered &lt;- res[order(res$padj), ]\ntop_genes &lt;- row.names(res_ordered)[1:50]\n```\n```{r}\n# Extract counts and normalize\ncounts &lt;- counts(deseq2.dds, normalized = TRUE)\ncounts_top &lt;- counts[top_genes, ]\n\n# Log-transform counts\nlog_counts_top &lt;- log2(counts_top + 1)\n\n# Generate heatmap\npheatmap(log_counts_top, scale = \"row\")\n```\n```{r}\nhead(deseq2.res)\n```\n```{r}\n# Count number of hits with adjusted p-value less then 0.05\ndim(deseq2.res[!is.na(deseq2.res$padj) & deseq2.res$padj &lt;= 0.05, ])\n\ntmp &lt;- deseq2.res\n# The main plot\nplot(tmp$baseMean, tmp$log2FoldChange, pch=20, cex=0.45, ylim=c(-3, 3), log=\"x\", col=\"darkgray\",\n     main=\"DEG Dessication  (pval &lt;= 0.05)\",\n     xlab=\"mean of normalized counts\",\n     ylab=\"Log2 Fold Change\")\n# Getting the significant points and plotting them again so they're a different color\ntmp.sig &lt;- deseq2.res[!is.na(deseq2.res$padj) & deseq2.res$padj &lt;= 0.05, ]\npoints(tmp.sig$baseMean, tmp.sig$log2FoldChange, pch=20, cex=0.45, col=\"red\")\n# 2 FC lines\nabline(h=c(-1,1), col=\"blue\")\n\n```"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "modules/blast.html",
    "href": "modules/blast.html",
    "title": "NCBI Blast",
    "section": "",
    "text": "/home/shared/ncbi-blast-2.11.0+/bin/makeblastdb \\\n-in ../data/uniprot_sprot_r2023_01.fasta \\\n-dbtype prot \\\n-out ../blastdb/uniprot_sprot_r2023_01\n/home/shared/ncbi-blast-2.11.0+/bin/blastx \\\n-query ../data/Ab_4denovo_CLC6_a.fa \\\n-db ../blastdb/uniprot_sprot_r2023_01 \\\n-out ../output/Ab_4-uniprot_blastx.tab \\\n-evalue 1E-20 \\\n-num_threads 20 \\\n-max_target_seqs 1 \\\n-outfmt 6"
  },
  {
    "objectID": "modules/blast.html#database-creation",
    "href": "modules/blast.html#database-creation",
    "title": "NCBI Blast",
    "section": "Database Creation",
    "text": "Database Creation\n\nObtain Fasta (UniProt/Swiss-Prot)\nThis is from here picur reviewe sequences I named based on the identify of the database given\n\ncurrent_time &lt;- format(Sys.time(), \"%B %d, %Y %H:%M:%S\")\ncat(\"current date and time is \", current_time)\n\n\ncd ../data\ncurl -O https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.fasta.gz\nmv uniprot_sprot.fasta.gz uniprot_sprot_r2023_04.fasta.gz\ngunzip -k uniprot_sprot_r2023_04.fasta.gz\n\n\n\nMaking the database\n\nmkdir ../blastdb\n/home/shared/ncbi-blast-2.11.0+/bin/makeblastdb \\\n-in ../data/uniprot_sprot_r2023_01.fasta \\\n-dbtype prot \\\n-out ../blastdb/uniprot_sprot_r2023_01"
  },
  {
    "objectID": "modules/blast.html#getting-the-query-fasta-file",
    "href": "modules/blast.html#getting-the-query-fasta-file",
    "title": "NCBI Blast",
    "section": "Getting the query fasta file",
    "text": "Getting the query fasta file\n\ncurl https://eagle.fish.washington.edu/cnidarian/Ab_4denovo_CLC6_a.fa \\\n-k \\\n&gt; ../data/Ab_4denovo_CLC6_a.fa\n\nExploring what fasta file\n\nhead -3 ../data/Ab_4denovo_CLC6_a.fa\n\n\necho \"How many sequences are there?\"\ngrep -c \"&gt;\" ../data/Ab_4denovo_CLC6_a.fa\n\n\n# Read FASTA file\nfasta_file &lt;- \"../data/Ab_4denovo_CLC6_a.fa\"  # Replace with the name of your FASTA file\nsequences &lt;- readDNAStringSet(fasta_file)\n\n# Calculate sequence lengths\nsequence_lengths &lt;- width(sequences)\n\n# Create a data frame\nsequence_lengths_df &lt;- data.frame(Length = sequence_lengths)\n\n# Plot histogram using ggplot2\nggplot(sequence_lengths_df, aes(x = Length)) +\n  geom_histogram(binwidth = 1, color = \"grey\", fill = \"blue\", alpha = 0.75) +\n  labs(title = \"Histogram of Sequence Lengths\",\n       x = \"Sequence Length\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n# Read FASTA file\nfasta_file &lt;- \"../data/Ab_4denovo_CLC6_a.fa\"\nsequences &lt;- readDNAStringSet(fasta_file)\n\n# Calculate base composition\nbase_composition &lt;- alphabetFrequency(sequences, baseOnly = TRUE)\n\n# Convert to data frame and reshape for ggplot2\nbase_composition_df &lt;- as.data.frame(base_composition)\nbase_composition_df$ID &lt;- rownames(base_composition_df)\nbase_composition_melted &lt;- reshape2::melt(base_composition_df, id.vars = \"ID\", variable.name = \"Base\", value.name = \"Count\")\n\n# Plot base composition bar chart using ggplot2\nggplot(base_composition_melted, aes(x = Base, y = Count, fill = Base)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", color = \"black\") +\n  labs(title = \"Base Composition\",\n       x = \"Base\",\n       y = \"Count\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"A\" = \"green\", \"C\" = \"blue\", \"G\" = \"yellow\", \"T\" = \"red\"))\n\n\n# Read FASTA file\nfasta_file &lt;- \"../data/Ab_4denovo_CLC6_a.fa\"\nsequences &lt;- readDNAStringSet(fasta_file)\n\n# Count CG motifs in each sequence\ncount_cg_motifs &lt;- function(sequence) {\n  cg_motif &lt;- \"CG\"\n  return(length(gregexpr(cg_motif, sequence, fixed = TRUE)[[1]]))\n}\n\ncg_motifs_counts &lt;- sapply(sequences, count_cg_motifs)\n\n# Create a data frame\ncg_motifs_counts_df &lt;- data.frame(CG_Count = cg_motifs_counts)\n\n# Plot CG motifs distribution using ggplot2\nggplot(cg_motifs_counts_df, aes(x = CG_Count)) +\n  geom_histogram(binwidth = 1, color = \"black\", fill = \"blue\", alpha = 0.75) +\n  labs(title = \"Distribution of CG Motifs\",\n       x = \"Number of CG Motifs\",\n       y = \"Frequency\") +\n  theme_minimal()"
  },
  {
    "objectID": "modules/blast.html#running-blastx",
    "href": "modules/blast.html#running-blastx",
    "title": "NCBI Blast",
    "section": "Running Blastx",
    "text": "Running Blastx\n\n~/applications/ncbi-blast-2.13.0+/bin/blastx \\\n-query ../data/Ab_4denovo_CLC6_a.fa \\\n-db ../blastdb/uniprot_sprot_r2023_01 \\\n-out ../output/Ab_4-uniprot_blastx.tab \\\n-evalue 1E-20 \\\n-num_threads 20 \\\n-max_target_seqs 1 \\\n-outfmt 6\n\n\nhead -2 ../output/Ab_4-uniprot_blastx.tab\n\n\necho \"Number of lines in output\"\nwc -l ../output/Ab_4-uniprot_blastx.tab"
  },
  {
    "objectID": "modules/blast.html#joining-blast-table-with-annoations.",
    "href": "modules/blast.html#joining-blast-table-with-annoations.",
    "title": "NCBI Blast",
    "section": "Joining Blast table with annoations.",
    "text": "Joining Blast table with annoations.\n\nPrepping Blast table for easy join\n\ntr '|' '\\t' &lt; ../output/Ab_4-uniprot_blastx.tab \\\n&gt; ../output/Ab_4-uniprot_blastx_sep.tab\n\nhead -1 ../output/Ab_4-uniprot_blastx_sep.tab"
  },
  {
    "objectID": "modules/blast.html#could-do-some-cool-stuff-in-r-here-reading-in-table",
    "href": "modules/blast.html#could-do-some-cool-stuff-in-r-here-reading-in-table",
    "title": "NCBI Blast",
    "section": "Could do some cool stuff in R here reading in table",
    "text": "Could do some cool stuff in R here reading in table\n\nbltabl &lt;- read.csv(\"../output/Ab_4-uniprot_blastx_sep.tab\", sep = '\\t', header = FALSE)\n\nspgo &lt;- read.csv(\"https://gannet.fish.washington.edu/seashell/snaps/uniprot_table_r2023_01.tab\", sep = '\\t', header = TRUE)\n\n\ndatatable(head(bltabl), options = list(scrollX = TRUE, scrollY = \"400px\", scrollCollapse = TRUE, paging = FALSE))\n\n\ndatatable(head(spgo), options = list(scrollX = TRUE, scrollY = \"400px\", scrollCollapse = TRUE, paging = FALSE))\n\n\ndatatable(\n  left_join(bltabl, spgo,  by = c(\"V3\" = \"Entry\")) %&gt;%\n  select(V1, V3, V13, Protein.names, Organism, Gene.Ontology..biological.process., Gene.Ontology.IDs) %&gt;% mutate(V1 = str_replace_all(V1, \n            pattern = \"solid0078_20110412_FRAG_BC_WHITE_WHITE_F3_QV_SE_trimmed\", replacement = \"Ab\"))\n)\n\n\nannot_tab &lt;-\n  left_join(bltabl, spgo,  by = c(\"V3\" = \"Entry\")) %&gt;%\n  select(V1, V3, V13, Protein.names, Organism, Gene.Ontology..biological.process., Gene.Ontology.IDs) %&gt;% mutate(V1 = str_replace_all(V1, \n            pattern = \"solid0078_20110412_FRAG_BC_WHITE_WHITE_F3_QV_SE_trimmed\", replacement = \"Ab\"))\n\n\n# Read dataset\ndataset &lt;- read.csv(\"../output/blast_annot_go.tab\", sep = '\\t')  # Replace with the path to your dataset\n\n# Select the column of interest\ncolumn_name &lt;- \"Organism\"  # Replace with the name of the column of interest\ncolumn_data &lt;- dataset[[column_name]]\n\n# Count the occurrences of the strings in the column\nstring_counts &lt;- table(column_data)\n\n# Convert to a data frame, sort by count, and select the top 10\nstring_counts_df &lt;- as.data.frame(string_counts)\ncolnames(string_counts_df) &lt;- c(\"String\", \"Count\")\nstring_counts_df &lt;- string_counts_df[order(string_counts_df$Count, decreasing = TRUE), ]\ntop_10_strings &lt;- head(string_counts_df, n = 10)\n\n# Plot the top 10 most common strings using ggplot2\nggplot(top_10_strings, aes(x = reorder(String, -Count), y = Count, fill = String)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", color = \"black\") +\n  labs(title = \"Top 10 Species hits\",\n       x = column_name,\n       y = \"Count\") +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  coord_flip()\n\n\ndata &lt;- read.csv(\"../output/blast_annot_go.tab\", sep = '\\t')\n\n# Rename the `Gene.Ontology..biological.process.` column to `Biological_Process`\ncolnames(data)[colnames(data) == \"Gene.Ontology..biological.process.\"] &lt;- \"Biological_Process\"\n\n# Separate the `Biological_Process` column into individual biological processes\ndata_separated &lt;- unlist(strsplit(data$Biological_Process, split = \";\"))\n\n# Trim whitespace from the biological processes\ndata_separated &lt;- gsub(\"^\\\\s+|\\\\s+$\", \"\", data_separated)\n\n# Count the occurrences of each biological process\nprocess_counts &lt;- table(data_separated)\nprocess_counts &lt;- data.frame(Biological_Process = names(process_counts), Count = as.integer(process_counts))\nprocess_counts &lt;- process_counts[order(-process_counts$Count), ]\n\n# Select the 20 most predominant biological processes\ntop_20_processes &lt;- process_counts[1:20, ]\n\n# Create a color palette for the bars\nbar_colors &lt;- rainbow(nrow(top_20_processes))\n\n# Create a staggered vertical bar plot with different colors for each bar\nbarplot(top_20_processes$Count, names.arg = rep(\"\", nrow(top_20_processes)), col = bar_colors,\n        ylim = c(0, max(top_20_processes$Count) * 1.25),\n        main = \"Occurrences of the 20 Most Predominant Biological Processes\", xlab = \"Biological Process\", ylab = \"Count\")\n\n\n# Create a separate plot for the legend\npng(\"../output/GOlegend.png\", width = 800, height = 600)\npar(mar = c(0, 0, 0, 0))\nplot.new()\nlegend(\"center\", legend = top_20_processes$Biological_Process, fill = bar_colors, cex = 1, title = \"Biological Processes\")\ndev.off()\n\n\nknitr::include_graphics(\"../output/GOlegend.png\")"
  },
  {
    "objectID": "modules/blast.html#navigating-annotation",
    "href": "modules/blast.html#navigating-annotation",
    "title": "NCBI Blast",
    "section": "Navigating Annotation",
    "text": "Navigating Annotation\nThe following is a stepwise example or annotation of a gene set using UniProt::Swiss-Prot (reviewed) such that Gene Ontology terms can be associated with each gene.\nIn this following chunk where the fasta file is downloaded the release is noted and the file name is modified accordingly.\n\ncd DRAFT_Funct_Enrich/annot\n\ncurl -O https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.fasta.gz\n\nmv uniprot_sprot.fasta.gz uniprot_sprot_r2023_02.fasta.gz\ngunzip -k uniprot_sprot_r2023_02.fasta.gz\n\nA protein blast database is then made.\n\n/home/shared/ncbi-blast-2.11.0+/bin/makeblastdb \\\n-in DRAFT_Funct_Enrich/annot/uniprot_sprot_r2023_02.fasta \\\n-dbtype prot \\\n-out DRAFT_Funct_Enrich/annot/uniprot_sprot_r2023_02\n\nIn a majority of cases you will want to annotate a gene set to get gene ontology information. If you are creating your own genome or transcriptome it should be rather straightforward to know what file to annotate. If using a widely studied system where there are publically available resources, it is advisable to use those as this is the best way to facilitate integration of data sets. In this case study we will be considering the Eastern oyster, (Crassostrea virginica) for which there is data at NCBI and Ensembl Metazoa. At NCBI there is both a GenBank and RefSeq assembly available.\nIn order to know which of the numerous fasta files should annotated with gene ontology information one should think downstream (or look to files already generated) to the identifiers in genesets that would be subject to functional enrichment tests.\nThe resulting fpkm count matrix for our case study is from an experiment where male and female oysters where exposed to low pH (and control) conditions. The count matrix is accessible here (csv). Hisat2/Stringtie was used to generate the count matrix with GCF_002022765.2_C_virginica-3.0_genomic.gff formatting thus responsible for gene naming. Specifically the naming format is as follows gene-LOC111099033,gene-LOC111099034,gene-LOC111099035.\nThe following fasta was selected for annotation: GCF_002022765.2_C_virginica-3.0_translated_cds.faa.gz\n\ncd DRAFT_Funct_Enrich/annot\n\ncurl -O https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/002/022/765/GCF_002022765.2_C_virginica-3.0/GCF_002022765.2_C_virginica-3.0_translated_cds.faa.gz\n\ngunzip -k GCF_002022765.2_C_virginica-3.0_translated_cds.faa.gz\n\n\nhead -2 DRAFT_Funct_Enrich/annot/GCF_002022765.2_C_virginica-3.0_translated_cds.faa\n\necho \"number of sequences\"\ngrep -c  \"&gt;\" DRAFT_Funct_Enrich/annot/GCF_002022765.2_C_virginica-3.0_translated_cds.faa\n\n&gt;lcl|NC_035780.1_prot_XP_022327646.1_1 [gene=LOC111126949] [db_xref=GeneID:111126949] [protein=UNC5C-like protein] [protein_id=XP_022327646.1] [location=join(30535..31557,31736..31887,31977..32565,32959..33204)] [gbkey=CDS]\nMTEVCYIWASSSTTVVICGIFFIVWRCFISIKKRASPLHGSSQQVCQTCQIEGHDFGEFQLSCRRQNTNVGYDLQGRRSD\nThis protein fasta is used as query for blast of uniprot_sprot database.\n\n/home/shared/ncbi-blast-2.11.0+/bin/blastp \\\n-query DRAFT_Funct_Enrich/annot/GCF_002022765.2_C_virginica-3.0_translated_cds.faa \\\n-db DRAFT_Funct_Enrich/annot/uniprot_sprot_r2023_02 \\\n-out DRAFT_Funct_Enrich/annot/Cvir_transcds-uniprot_blastp.tab \\\n-evalue 1E-20 \\\n-num_threads 40 \\\n-max_target_seqs 1 \\\n-outfmt 6\n\nHere is what the output file looks like, and at this point we want to get the UniProt Accession number for each gene\n\nhead -2 DRAFT_Funct_Enrich/annot/Cvir_transcds-uniprot_blastp.tab\n\n\nblast &lt;- read.csv(\"DRAFT_Funct_Enrich/annot/Cvir_transcds-uniprot_blastp.tab\", sep = '\\t', header = FALSE)\n\nConvert fasta to tab\n\nperl -e '$count=0; $len=0; while(&lt;&gt;) {s/\\r?\\n//; s/\\t/ /g; if (s/^&gt;//) { if ($. != 1) {print \"\\n\"} s/ |$/\\t/; $count++; $_ .= \"\\t\";} else {s/ //g; $len += length($_)} print $_;} print \"\\n\"; warn \"\\nConverted $count FASTA records in $. lines to tabular format\\nTotal sequence length: $len\\n\\n\";' \\\nDRAFT_Funct_Enrich/annot/GCF_002022765.2_C_virginica-3.0_translated_cds.faa &gt; DRAFT_Funct_Enrich/annot/GCF_002022765.2_C_virginica-3.0_translated_cds.tab\n\n\nhead -1 DRAFT_Funct_Enrich/annot/GCF_002022765.2_C_virginica-3.0_translated_cds.tab\n\n\ncdsftab &lt;- read.csv(\"DRAFT_Funct_Enrich/annot/GCF_002022765.2_C_virginica-3.0_translated_cds.tab\", sep = '\\t', header = FALSE, row.names=NULL)\n\nNow we can take the two data frames: A) blast output of taking protein fasta and comparing to uniprot_swiss-prot and B) a tabular version of same fasta file that has ID numbers of importance. Note this importance was determined based on what we want to use down stream.\n\ng.spid &lt;- left_join(blast, cdsftab, by = \"V1\") %&gt;%\n  mutate(gene = str_extract(V2.y, \"(?&lt;=\\\\[gene=)\\\\w+\")) %&gt;%\n  select(gene, V11, V2.x) %&gt;%\n  mutate(SPID = str_extract(V2.x, \"(?&lt;=\\\\|)[^\\\\|]*(?=\\\\|)\")) %&gt;%\n  distinct(gene, SPID, .keep_all = TRUE)\n\nLet’s break it down step by step:\n\ng.spid &lt;- left_join(blast, cdsftab, by = \"V1\") - This line is using the left_join() function from dplyr to merge the blast and cdsftab datasets by the column “V1”. A left join retains all the rows in the blast data frame and appends the matching rows in the cdsftab data frame. If there is no match, the result is NA. The result of this operation is assigned to the g.spid object.\nmutate(gene = str_extract(V2.y, \"(?&lt;=\\\\[gene=)\\\\w+\")) - This line is using the mutate() function from dplyr to add a new column called “gene” to the data frame. The new column is created by extracting substrings from the “V2.y” column based on the given regular expression pattern \"(?&lt;=\\\\[gene=)\\\\w+\". This regular expression matches and extracts any word (sequence of word characters, i.e., alphanumeric and underscore) that comes after “[gene=”.\nselect(gene, V11, V2.x) - This line is using the select() function from dplyr to keep only the specified columns (“gene”, “V11”, and “V2.x”) in the data frame.\nmutate(SPID = str_extract(V2.x, \"(?&lt;=\\\\|)[^\\\\|]*(?=\\\\|)\")) - Again, the mutate() function is used to add another new column named “SPID”. This column is created by extracting substrings from the “V2.x” column. The regular expression \"(?&lt;=\\\\|)[^\\\\|]*(?=\\\\|)\" is designed to extract any character(s) that is/are surrounded by “|” (pipe symbol). This is a common format for delimited strings.\ndistinct(gene, SPID, .keep_all = TRUE) - This line is using the distinct() function from dplyr to remove duplicate rows based on the “gene” and “SPID” columns. The .keep_all = TRUE argument means that all other columns are also kept in the result, not just the “gene” and “SPID” columns.\n\nThe resulting g.spid data frame should have unique rows with respect to the “gene” and “SPID” columns, and it should contain these two new columns, “gene” and “SPID”, extracted from the original data based on specific string patterns.\nNow lets just write out SPIDs.\n\nleft_join(blast, cdsftab, by = \"V1\") %&gt;%\n  mutate(gene = str_extract(V2.y, \"(?&lt;=\\\\[gene=)\\\\w+\")) %&gt;%\n  select(gene, V11, V2.x) %&gt;%\n  mutate(SPID = str_extract(V2.x, \"(?&lt;=\\\\|)[^\\\\|]*(?=\\\\|)\")) %&gt;%\n  distinct(gene, SPID, .keep_all = TRUE) %&gt;%\n  select(SPID) %&gt;%\n  write.table(file = \"DRAFT_Funct_Enrich/annot/SPID.txt\", sep = \"\\t\", row.names = FALSE, quote = FALSE\n ) \n\nWith a list of matching Swiss-Prot IDs, (technically UniProt Accession number) we can go back to https://www.uniprot.org and grab corresponding GO terms. This can be done via a web or using Python API.\nUsing Web\nUsing ID Mapping\n\n\n\nid\n\n\n\n\n\nfinished\n\n\nNow will customize columns to get GO IDs.\n\n\n\ncustcol\n\n\n\nhead -2 DRAFT_Funct_Enrich/annot/uniprotGO.tab\n\nFinally we can join table to get “LOCIDs” the notation for our DEGs, with GO terms.\n\ngo &lt;- read.csv(\"DRAFT_Funct_Enrich/annot/uniprotGO.tab\", sep = '\\t', header = TRUE, row.names=NULL)\n\n\nleft_join(g.spid, go, by = c(\"SPID\" = \"Entry\")) %&gt;%\n  select(gene,Gene.Ontology.IDs) %&gt;%\n  write.table(file = \"DRAFT_Funct_Enrich/annot/geneGO.txt\", sep = \"\\t\", row.names = FALSE, quote = FALSE\n  )\n\n\nhead DRAFT_Funct_Enrich/annot/geneGO.txt\n\nUsing API\npython3 DRAFT_Funct_Enrich/annot/uniprot-retrieval.py DRAFT_Funct_Enrich/annot/SPID.txt"
  }
]